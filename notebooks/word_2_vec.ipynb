{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import polars as pl\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "train = pl.read_parquet('../data/train.parquet')\n",
    "test = pl.read_parquet('../data/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df =  pl.concat([train, test]).groupby('session').agg(\n",
    "    pl.col('aid').alias('sentence')\n",
    ")\n",
    "\n",
    "sentences = sentences_df['sentence'].to_list()\n",
    "del sentences_df; gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 18min 54s, sys: 1min 34s, total: 3h 20min 29s\n",
      "Wall time: 1h 58min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "w2vec = Word2Vec(sentences=sentences, vector_size= 64, window = 3, negative = 8, ns_exponent = 0.2, sg = 1, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 7.09 s, total: 2min 35s\n",
      "Wall time: 42.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "aid2idx = {aid: i for i, aid in enumerate(w2vec.wv.index_to_key)}\n",
    "index = AnnoyIndex(64, 'euclidean')\n",
    "\n",
    "for aid, idx in aid2idx.items():\n",
    "    index.add_item(idx, w2vec.wv.vectors[idx])\n",
    "    \n",
    "index.build(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "\n",
    "session_types = ['clicks', 'carts', 'orders']\n",
    "test_session_AIDs = test.to_pandas().reset_index(drop=True).groupby('session')['aid'].apply(list)\n",
    "test_session_types = test.to_pandas().reset_index(drop=True).groupby('session')['type'].apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "type_weight_multipliers = {0: 1, 1: 6, 2: 3}\n",
    "\n",
    "session_num = len(test_session_AIDs)\n",
    "\n",
    "for AIDs, types in zip(test_session_AIDs[:session_num], test_session_types[:session_num]):\n",
    "    if len(AIDs) >= 20:\n",
    "        # if we have enough aids (over equals 20) we don't need to look for candidates! we just use the old logic\n",
    "        weights=np.logspace(0.1,1,len(AIDs),base=2, endpoint=True)-1\n",
    "        aids_temp=defaultdict(lambda: 0)\n",
    "        for aid,w,t in zip(AIDs,weights,types): \n",
    "            aids_temp[aid]+= w * type_weight_multipliers[t]\n",
    "            \n",
    "        sorted_aids=[k for k, v in sorted(aids_temp.items(), key=lambda item: -item[1])]\n",
    "        labels.append(sorted_aids[:20])\n",
    "    else:\n",
    "        # here we don't have 20 aids to output -- we will use word2vec embeddings to generate candidates!\n",
    "        AIDs = list(dict.fromkeys(AIDs[::-1]))\n",
    "        \n",
    "        # let's grab the up to 3 recent aids\n",
    "        recent_len = max(min(3,len(AIDs)),1)\n",
    "        \n",
    "        # how many aids for each aid\n",
    "        AIDs_num = round((20-len(AIDs))/recent_len) + 2\n",
    "        \n",
    "        # let's look for some neighbors!        \n",
    "        nns_it = []\n",
    "        for it in range(0,recent_len):\n",
    "            nns_it += [w2vec.wv.index_to_key[i] for i in index.get_nns_by_item(aid2idx[AIDs[it]], AIDs_num)[1:]]\n",
    "        \n",
    "        # select repeating and unique neighbors\n",
    "        nns_repeated = [item for item, count in collections.Counter(nns_it).items() if count > 1]\n",
    "        nns_once = [item for item, count in collections.Counter(nns_it).items() if count == 1]\n",
    "\n",
    "        # prepare selection\n",
    "        nns = (nns_repeated+nns_once)[:20]\n",
    "        labels.append((AIDs+nns)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_as_strings = [' '.join([str(l) for l in lls]) for lls in labels]\n",
    "\n",
    "predictions = pd.DataFrame(data={'session_type': test_session_AIDs.index, 'labels': labels_as_strings})\n",
    "\n",
    "prediction_dfs = []\n",
    "\n",
    "for st in session_types:\n",
    "    modified_predictions = predictions.copy()\n",
    "    modified_predictions.session_type = modified_predictions.session_type.astype('str') + f'_{st}'\n",
    "    prediction_dfs.append(modified_predictions)\n",
    "\n",
    "submission = pd.concat(prediction_dfs).reset_index(drop=True)\n",
    "submission.to_csv('../data/sample_submission.csv', index=False)\n",
    "\n",
    "del labels, labels_as_strings, predictions, prediction_dfs\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "360acb43446fba8ca899ac977f179740967a7b7c03e384545efc55acd2382d4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
